{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a1e0fb-31aa-462d-9860-b6d7963d3ce0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---\n",
    "Author: Mustapha Bouhsen <br>\n",
    "[LinkedIn](https://www.linkedin.com/in/mustapha-bouhsen/)<br>\n",
    "[Git](https://github.com/mus514)<br>\n",
    "Date: February 2, 2024<br>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af7d6dd-6d88-4f1d-8d94-353b6b5db53d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc5e317-cb99-4d6b-a211-a1ab8bb20e94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Load files from Azure blob storage : Set the data location and type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18aa4b01-e150-48ef-a636-87385f306558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# storage_account_name = \"mymlprojects\"\n",
    "# storage_key = \"?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2024-03-09T09:14:29Z&st=2024-02-03T01:14:29Z&spr=https&sig=v%2Bmvq02eWWEzGfaXqGJ%2F8BJiTJrD3PPGS4eL66SIsC8%3D\"\n",
    "\n",
    "# container_name = \"prod\"\n",
    "# mount_point = \"/mnt/prod\"\n",
    "\n",
    "# dbutils.fs.mount(\n",
    "#   source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "#   mount_point = mount_point,\n",
    "#   extra_configs = {f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\":storage_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74ff9f7-f90e-46e0-a1ac-e3dcd35d6648",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def json_to_spark_df(file_path, schema):\n",
    "    \"\"\"\n",
    "    Reads a JSON file into a Spark DataFrame, processes the data, and returns a new Spark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the JSON file.\n",
    "    - schema (pyspark.sql.types.StructType): The schema of the resulting Spark DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Spark DataFrame containing the processed data.\n",
    "\n",
    "    \"\"\"\n",
    "    # Read JSON file into a Spark DataFrame\n",
    "    data = spark.read.json(file_path).collect()\n",
    "\n",
    "    # Extract the actual data from the collected DataFrame\n",
    "    # Assuming the JSON file contains a single row of data\n",
    "    df = pd.DataFrame(data[0][1].asDict()).T.reset_index()\n",
    "\n",
    "    # Create a Spark DataFrame from the pandas DataFrame, specifying the schema\n",
    "    return spark.createDataFrame(df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ca3f8d-013d-4956-b169-1a982b29dd74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_files_paths_from_folders(folder_path, endsWith=None):\n",
    "    \"\"\"\n",
    "    Recursively retrieves the paths of all files within the specified folder and its subfolders.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the folder for which file paths are to be retrieved.\n",
    "    - endsWith (str, optional): The suffix to filter files by. Defaults to \".parquet\".\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list containing the paths of all files within the specified folder and its subfolders that end with the specified suffix.\n",
    "    \"\"\"\n",
    "    # Get the list of paths (files and subfolders) within the specified folder\n",
    "    paths = dbutils.fs.ls(folder_path)\n",
    "\n",
    "    # Initialize an empty list to store file paths\n",
    "    my_paths = []\n",
    "\n",
    "    # Iterate through the paths to identify files and subfolders\n",
    "    for key in paths:\n",
    "        # Check if the current path corresponds to a file\n",
    "        if key.isFile():\n",
    "            # If it's a file, append its path to the list\n",
    "            my_paths.append(key[0])\n",
    "        else:\n",
    "            # If it's a subfolder, recursively call the function to get file paths within the subfolder\n",
    "            my_paths = my_paths + get_files_paths_from_folders(key[0])\n",
    "\n",
    "    if endsWith != None:\n",
    "        # Filter the list of paths to include only those ending with the specified suffix\n",
    "        my_paths = [path for path in my_paths if path.endswith(endsWith)]\n",
    "\n",
    "    # Return the final list of file paths\n",
    "    return my_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f784adef-17bc-49e9-996c-30817685cb17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delete_contents_recursively(folder_path):\n",
    "    # List all files and subdirectories in the folder\n",
    "    contents = dbutils.fs.ls(folder_path)\n",
    "\n",
    "    # Delete each file and subdirectory\n",
    "    for content in contents:\n",
    "        if content.isDir():\n",
    "            # Recursively delete contents of subfolder\n",
    "            delete_contents_recursively(content.path)\n",
    "        else:\n",
    "            # Delete file\n",
    "            dbutils.fs.rm(content.path)\n",
    "\n",
    "    # After deleting all contents, delete the folder itself\n",
    "    dbutils.fs.rm(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be82b42c-20cc-46fd-9c98-4bea31464e17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_and_transform_to_parquet(files_paths, prod_folder_path, stock_name):\n",
    "    \"\"\"\n",
    "    Ingests data from specified files, extracts date information, and transforms it to Parquet format.\n",
    "\n",
    "    Parameters:\n",
    "    - files_paths (list): List of file paths to be ingested.\n",
    "    - prod_folder_path (str): Base path for the Parquet output.\n",
    "    - stock_name (str) : the stock name\n",
    "    \"\"\"\n",
    "    for file in files_paths:\n",
    "        # Extract date information from the file path\n",
    "        date_file = file.split(\"/\")[-3:-1]\n",
    "        year = int(date_file[0].split(\"=\")[1])\n",
    "        month = int(date_file[1].split(\"=\")[1])\n",
    "\n",
    "        # Build the destination Parquet file path\n",
    "        prod_file_path = f'{prod_folder_path}{stock_name.lower()}/year={year}/month={\"{:02}\".format(month)}/{stock_name}.parquet'\n",
    "        # Copy the file to the Parquet destination\n",
    "        dbutils.fs.cp(file, prod_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c8842f-8c1f-428e-a1e3-a1869f5ac63c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# Set the raw and the prod folder paths\n",
    "#-----------------------------------------\n",
    "raw_folder_path = \"/mnt/raw/\"\n",
    "prod_folder_path = \"/mnt/prod/\"\n",
    "\n",
    "raw_files_paths = [file.path for file in dbutils.fs.ls(raw_folder_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e79333-8bdc-4208-aea0-c99f2c5d831f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# The schema\n",
    "#-----------------------------------------\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"open\", StringType(), True),\n",
    "    StructField(\"high\", StringType(), True),\n",
    "    StructField(\"low\", StringType(), True),\n",
    "    StructField(\"close\", StringType(), True),\n",
    "    StructField(\"volume\", StringType(), True)\n",
    "])\n",
    "\n",
    "col_float = [\"open\", \"high\", \"low\", \"close\", \"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6a8fb7-c42a-4034-a776-77639ccc634d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_stocks(raw_files_paths):\n",
    "    try:\n",
    "        for file in raw_files_paths:\n",
    "            stock_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "            print(f'process start for {stock_name}')\n",
    "            # Load the data\n",
    "            df = json_to_spark_df(file, schema=schema)\n",
    "\n",
    "            #Convert the columns type\n",
    "            # Date column\n",
    "            df = df.withColumn(\"date\", F.col(\"date\").cast(DateType()))\n",
    "            # Float columns\n",
    "            for col in col_float:\n",
    "                df = df.withColumn(col, F.col(col).cast(FloatType()))\n",
    "            \n",
    "            # Add year and month to make partion\n",
    "            df = df.withColumn(\"year\", F.year(F.col(\"date\")))\n",
    "            df = df.withColumn(\"month\", F.month(F.col(\"date\")))\n",
    "\n",
    "            # Temp folder to save temp parquet files\n",
    "            temp_folder = prod_folder_path+\"temp/\"\n",
    "            # Partion files in folders by year and month\n",
    "            df.write.partitionBy([\"year\", \"month\"]).mode(\"overwrite\").parquet(temp_folder)\n",
    "            # Delet the succes file\n",
    "            dbutils.fs.rm(temp_folder+\"_SUCCESS\")\n",
    "\n",
    "            # get all files path ending with .parquet\n",
    "            files_paths = get_files_paths_from_folders(temp_folder, \".parquet\")\n",
    "            \n",
    "            # Copy parquet files to final destination\n",
    "            ingest_and_transform_to_parquet(files_paths, prod_folder_path, stock_name)\n",
    "\n",
    "            # delete the temp folder\n",
    "            delete_contents_recursively(temp_folder)\n",
    "            print(f'process end for {stock_name}')\n",
    "            print('--------------------------------')  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"The error is: \",e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e160190d-cdda-4370-990b-45b37ce8e528",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process start for AAPL\nprocess end for AAPL\n--------------------------------\nprocess start for AMZN\nprocess end for AMZN\n--------------------------------\nprocess start for MSFT\nprocess end for MSFT\n--------------------------------\nprocess start for TSLA\nprocess end for TSLA\n--------------------------------\n"
     ]
    }
   ],
   "source": [
    "process_stocks(raw_files_paths)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4493154419901656,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "db-daily_process_stocks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

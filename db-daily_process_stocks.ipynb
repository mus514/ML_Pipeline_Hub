{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a1e0fb-31aa-462d-9860-b6d7963d3ce0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---\n",
    "Author: Mustapha Bouhsen <br>\n",
    "[LinkedIn](https://www.linkedin.com/in/mustapha-bouhsen/)<br>\n",
    "[Git](https://github.com/mus514)<br>\n",
    "Date: February 2, 2024<br>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfaf51ca-cd4a-4f45-a48b-49c0ed5a1a9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Load files from Azure blob storage : Set the data location and type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c88d764-d22d-401e-a942-73108da2fdea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run Repos/mustapha.bouhsen@hec.ca/ML_Pipeline_Hub/library/daily_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18aa4b01-e150-48ef-a636-87385f306558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# storage_account_name = \"mymlprojects\"\n",
    "# storage_key = \"?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2024-03-09T09:14:29Z&st=2024-02-03T01:14:29Z&spr=https&sig=v%2Bmvq02eWWEzGfaXqGJ%2F8BJiTJrD3PPGS4eL66SIsC8%3D\"\n",
    "\n",
    "# container_name = \"prod\"\n",
    "# mount_point = \"/mnt/prod\"\n",
    "\n",
    "# dbutils.fs.mount(\n",
    "#   source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "#   mount_point = mount_point,\n",
    "#   extra_configs = {f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\":storage_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c8842f-8c1f-428e-a1e3-a1869f5ac63c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# Set the raw and the prod folder paths\n",
    "#-----------------------------------------\n",
    "raw_folder_path = \"/mnt/raw/\"\n",
    "prod_folder_path = \"/mnt/prod/\"\n",
    "\n",
    "raw_files_paths = [file.path for file in dbutils.fs.ls(raw_folder_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e79333-8bdc-4208-aea0-c99f2c5d831f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# The schema\n",
    "#-----------------------------------------\n",
    "schema = StructType([\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"open\", FloatType(), True),\n",
    "    StructField(\"high\", FloatType(), True),\n",
    "    StructField(\"low\", FloatType(), True),\n",
    "    StructField(\"close\", FloatType(), True),\n",
    "    StructField(\"adj_close\", FloatType(), True),\n",
    "    StructField(\"volume\", FloatType(), True)\n",
    "])\n",
    "\n",
    "col_float = [\"open\", \"high\", \"low\", \"close\", \"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8137dcb3-01f5-4106-bc25-277e1ab06827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"True\").schema(schema).csv(raw_files_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6a8fb7-c42a-4034-a776-77639ccc634d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_stocks(raw_files_paths):\n",
    "    try:\n",
    "        for file_path in raw_files_paths:\n",
    "            stock_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "            print(f'process start for {stock_name}')\n",
    "            # Load the data\n",
    "            df = spark.read.option(\"header\", \"True\").schema(schema).csv(file_path)\n",
    "\n",
    "            # Add year and month to make partion\n",
    "            df = df.withColumn(\"year\", F.year(F.col(\"date\")))\n",
    "            df = df.withColumn(\"month\", F.month(F.col(\"date\")))\n",
    "\n",
    "            # Temp folder to save temp parquet files\n",
    "            temp_folder = prod_folder_path+\"temp/\"\n",
    "            # Partion files in folders by year and month\n",
    "            df.write.partitionBy([\"year\", \"month\"]).mode(\"overwrite\").parquet(temp_folder)\n",
    "            # Delet the succes file\n",
    "            dbutils.fs.rm(temp_folder+\"_SUCCESS\")\n",
    "\n",
    "            # get all files path ending with .parquet\n",
    "            files_paths = get_files_paths_from_folders(temp_folder, \".parquet\")\n",
    "            \n",
    "            # Copy parquet files to final destination\n",
    "            ingest_and_transform_to_parquet(files_paths, prod_folder_path, stock_name)\n",
    "\n",
    "            # delete the temp folder\n",
    "            delete_contents_recursively(temp_folder)\n",
    "            print(f'process end for {stock_name}')\n",
    "            print('--------------------------------')  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"The error is: \",e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e160190d-cdda-4370-990b-45b37ce8e528",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process start for AAPL\nThe error is:  name 'file_path' is not defined\n"
     ]
    }
   ],
   "source": [
    "process_stocks(raw_files_paths)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4493154419901656,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "db-daily_process_stocks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

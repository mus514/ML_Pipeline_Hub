{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a1e0fb-31aa-462d-9860-b6d7963d3ce0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---\n",
    "Author: Mustapha Bouhsen <br>\n",
    "[LinkedIn](https://www.linkedin.com/in/mustapha-bouhsen/)<br>\n",
    "[Git](https://github.com/mus514)<br>\n",
    "Date: February 2, 2024<br>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af7d6dd-6d88-4f1d-8d94-353b6b5db53d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc5e317-cb99-4d6b-a211-a1ab8bb20e94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Load files from Azure blob storage : Set the data location and type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18aa4b01-e150-48ef-a636-87385f306558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# storage_account_name = \"mymlprojects\"\n",
    "# storage_key = \"?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2024-03-09T09:14:29Z&st=2024-02-03T01:14:29Z&spr=https&sig=v%2Bmvq02eWWEzGfaXqGJ%2F8BJiTJrD3PPGS4eL66SIsC8%3D\"\n",
    "\n",
    "# container_name = \"prod\"\n",
    "# mount_point = \"/mnt/prod\"\n",
    "\n",
    "# dbutils.fs.mount(\n",
    "#   source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "#   mount_point = mount_point,\n",
    "#   extra_configs = {f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\":storage_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c8842f-8c1f-428e-a1e3-a1869f5ac63c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# Set the the raw and the prof folder paths\n",
    "#-----------------------------------------\n",
    "raw_folder_path = \"/mnt/raw/\"\n",
    "prod_folder_path = \"/mnt/prod/\"\n",
    "\n",
    "raw_files_paths = [file.path for file in dbutils.fs.ls(raw_folder_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e79333-8bdc-4208-aea0-c99f2c5d831f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# The schema\n",
    "#-----------------------------------------\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"open\", StringType(), True),\n",
    "    StructField(\"high\", StringType(), True),\n",
    "    StructField(\"low\", StringType(), True),\n",
    "    StructField(\"close\", StringType(), True),\n",
    "    StructField(\"volume\", StringType(), True)\n",
    "])\n",
    "\n",
    "col_float = [\"open\", \"high\", \"low\", \"close\", \"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f10868c-340c-4622-a5fe-f9d8ec2bc6eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# Load the json into spark data frame \n",
    "#-----------------------------------------\n",
    "data = spark.read.json(raw_files_paths[1]).collect()\n",
    "df = pd.DataFrame(data[0][1].asDict()).T.reset_index()\n",
    "df = spark.createDataFrame(df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b439f897-6eb8-443c-8ba4-fa8085685523",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# Convert the columns type\n",
    "#-----------------------------------------\n",
    "# date column\n",
    "df = df.withColumn(\"date\", F.col(\"date\").cast(DateType()))\n",
    "# Float columns\n",
    "for col in col_float:\n",
    "    df = df.withColumn(col, F.col(col).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2907dc67-35a3-4c6b-90b4-9bc283a34f64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908491e1-7c19-4536-9b34-d8c0d77dddb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"year\", F.year(F.col(\"date\")))\n",
    "df = df.withColumn(\"month\", F.month(F.col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a6d99b-122c-4001-96cf-53f45821f973",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39e3101-95d5-450d-804c-5688b340e446",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df.write.partitionBy([\"year\", \"month\"]).mode(\"overwrite\").parquet(prod_folder_path+\"/tempo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6a8fb7-c42a-4034-a776-77639ccc634d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for file in raw_files_paths:\n",
    "#     # Load the data\n",
    "#     data = spark.read.json(file).collect()\n",
    "#     df = pd.DataFrame(data[0][1].asDict()).T.reset_index()\n",
    "#     df = spark.createDataFrame(df, schema=schema)\n",
    "\n",
    "#     #Convert the columns type\n",
    "#     # Date column\n",
    "#     df = df.withColumn(\"date\", F.col(\"date\").cast(DateType()))\n",
    "#     # Float columns\n",
    "#     for col in col_float:\n",
    "#         df = df.withColumn(col, F.col(col).cast(FloatType()))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b51b32-032d-4cf3-98db-ddf71d694ecc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_folder = prod_folder_path+\"/tempo/\"\n",
    "dbutils.fs.rm(temp_folder+\"_SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc82bdee-acca-478d-b49e-90bf78104a91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_files_paths_from_folders(folder_path):\n",
    "    \"\"\"\n",
    "    Recursively retrieves the paths of all files within the specified folder and its subfolders.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the folder for which file paths are to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list containing the paths of all files within the specified folder and its subfolders.\n",
    "    \"\"\"\n",
    "    # Get the list of paths (files and subfolders) within the specified folder\n",
    "    paths = dbutils.fs.ls(folder_path)\n",
    "\n",
    "    # Initialize an empty list to store file paths\n",
    "    my_path = []\n",
    "\n",
    "    # Iterate through the paths to identify files and subfolders\n",
    "    for key in paths:\n",
    "        # Check if the current path corresponds to a file\n",
    "        if key.isFile():\n",
    "            # If it's a file, append its path to the list\n",
    "            my_path.append(key[0])\n",
    "        else:\n",
    "            # If it's a subfolder, recursively call the function to get file paths within the subfolder\n",
    "            my_path = my_path + get_files_paths_from_folders(key[0])\n",
    "\n",
    "    # Return the final list of file paths\n",
    "    return my_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d3edd94-4e52-40e4-885d-2c84b7553e8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_files_paths_from_folders(temp_folder)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0990bf0c-0a90-4fcb-9084-071a0a7dad02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(dbutils.fs.ls(temp_folder)[0][0])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "db-daily_process_stocks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
